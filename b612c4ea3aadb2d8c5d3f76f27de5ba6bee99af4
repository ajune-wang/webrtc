{
  "comments": [
    {
      "key": {
        "uuid": "80c63202_dcb8aaee",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 9
      },
      "lineNbr": 0,
      "author": {
        "id": 5053
      },
      "writtenOn": "2020-08-13T20:38:08Z",
      "side": 1,
      "message": "PTAL.",
      "revId": "b612c4ea3aadb2d8c5d3f76f27de5ba6bee99af4",
      "serverId": "58829da1-049c-39fb-b951-ebdcd0984767",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "c18a0f7e_ba6c6b32",
        "filename": "rtc_base/physical_socket_server.cc",
        "patchSetId": 9
      },
      "lineNbr": 1225,
      "author": {
        "id": 5053
      },
      "writtenOn": "2020-08-13T20:38:08Z",
      "side": 1,
      "message": "We already have this mechanism to deal with dispatchers being added/removed during the select() loop. So I think this could be used for epoll as well; if a dispatcher is removed and a new one is added with the same address, the new one wouldn\u0027t be added until the next iteration, and any events produced by the old one would be ignored.\n\nHowever, we still need some custom structure to deal with the new test I added (because we need the ability to mark an entry as invalid without modifying the list). And I like the key implementation personally, it makes lookup O(1) and gives added peace of mind in case there\u0027s a case of the ABA problem I haven\u0027t thought of, or could be introduced in the future.",
      "revId": "b612c4ea3aadb2d8c5d3f76f27de5ba6bee99af4",
      "serverId": "58829da1-049c-39fb-b951-ebdcd0984767",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "0f976514_e8ad20f7",
        "filename": "rtc_base/physical_socket_server.cc",
        "patchSetId": 9
      },
      "lineNbr": 1225,
      "author": {
        "id": 13594
      },
      "writtenOn": "2020-08-13T22:11:16Z",
      "side": 1,
      "message": "\u003e We already have this mechanism to deal with dispatchers being added/removed during the select() loop. So I think this could be used for epoll as well; if a dispatcher is removed and a new one is added with the same address, the new one wouldn\u0027t be added until the next iteration, and any events produced by the old one would be ignored.\n\n1. The dispatched can be removed/added while the event loop thread has not yet locked crit_ after return from epoll_wait. processing_dispatchers_ is not set at that point. The reported events may be related to the dispatcher that has already been removed.\n\n2. We may hit the limit on the number of events reported from epoll_wait. So it is possible that events pertaining to a removed dispatcher will be delivered with a delay of one or more epoll_wait loop iterations. This makes the processing_dispatchers_ check ineffective.",
      "parentUuid": "c18a0f7e_ba6c6b32",
      "revId": "b612c4ea3aadb2d8c5d3f76f27de5ba6bee99af4",
      "serverId": "58829da1-049c-39fb-b951-ebdcd0984767",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "46829394_a5878ad7",
        "filename": "rtc_base/physical_socket_server.cc",
        "patchSetId": 9
      },
      "lineNbr": 1225,
      "author": {
        "id": 5053
      },
      "writtenOn": "2020-08-14T00:40:57Z",
      "side": 1,
      "message": "Good points. I tend to forget about the critical section because, as far as I know, WebRTC only interacts with sockets on one thread.\n\nJust curious, is #2 a problem even if we do EPOLL_CTL_DEL with the old socket before calling epoll_wait again?\n\nAlso, unless I\u0027m mistaken, we still have problem #2 with the normal select loop; a socket could be closed and a new one created with the same handle during the select() call. To address this, we could add an \"added_to_fd_set\" flag to Dispatcher which would only be set to true once FD_SET is called. Can you think of anything more elegant?\n\nGuess this is a problem for Windows as well. Though now that I\u0027m finally looking at it, I don\u0027t understand the Windows code *at all*. All actual sockets will return WSA_INVALID_EVENT for GetWSAEvent, yet they\u0027re still added to the event list. Instead of each socket having its own event, WSAEventSelect is called with events[0] for every socket. So WSAWaitForMultipleEvents is actually only waiting for a single event which indicates \"WakeUp was called or something happened to one of the sockets\". Then *all* sockets are iterated (even if WakeUp was called and nothing actually happened to any socket!).\n\nIs this is busted as it seems? Been a while since I worked with Windows sockets but this seems to confirm: https://stackoverflow.com/questions/19936131/wsaeventselect-one-event-multiple-sockets",
      "parentUuid": "0f976514_e8ad20f7",
      "revId": "b612c4ea3aadb2d8c5d3f76f27de5ba6bee99af4",
      "serverId": "58829da1-049c-39fb-b951-ebdcd0984767",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "b6551997_f1822ab7",
        "filename": "rtc_base/physical_socket_server.cc",
        "patchSetId": 9
      },
      "lineNbr": 1225,
      "author": {
        "id": 13594
      },
      "writtenOn": "2020-08-14T11:16:10Z",
      "side": 1,
      "message": "\u003e I tend to forget about the critical section because, as far as I know, WebRTC only interacts with sockets on one thread.\n\nIf so then crit_ is probably not needed. I can\u0027t be sure if that\u0027s the case, though.\n\n\u003e Just curious, is #2 a problem even if we do EPOLL_CTL_DEL with the old socket before calling epoll_wait again?\n\nProbably not. epoll_wait description suggests that it uses round robin selection of the file descriptors to report events, rather than internal queueing of events. This means that if a file descriptor is removed from epoll then it will no longer be reported in events after the removal. So, I probably overthought #2 wrt. epoll.\n\n\u003e Also, unless I\u0027m mistaken, we still have problem #2 with the normal select loop; a socket could be closed and a new one created with the same handle during the select() call.\n\nIt\u0027s more like #1, but yes, it seems possible. I would probably try a solution that reuses the dispatcher-\u003ekey_ infrastructure. For example, make an array/vector of keys that you initialize along with fdsets in WaitSelect with keys of dispatchers that you intend to select on. Then, when select returns, iterate over that array and lookup for keys in dispatchers_. If the key is found then the dispatcher is valid and you can carry on. If not, it has been removed and has to be skipped from processing.\n\nThe array could be made a member of PhysicalSocketServer to cache memory.\n\n\u003e Guess this is a problem for Windows as well.\n\nI have little knowledge how Windows sockets work. But it seems to do something along the lines of what I\u0027ve described above, only in two vectors, without keys and memory caching. It seems to be vulnerable to the ABA problem (i.e. it saves the dispatcher pointer to event_owners, which it later looks up in dispatchers_; that pointer may get reused by a different dispatcher). It looks like it could be ported to the same key infrastructure, and that should solve the ABA problem. Also, I would move the vectors to PhysicalSocketServer members to cache memory.\n\nI can\u0027t comment about the GetWSAEvent part much. It looks like the code multiplexes all sockets on a single WSAEVENT by calling WSAEventSelect, and adds all non-socket dispatchers to the list as is. So you have a socket-specific WSAEVENT entry first in the list and then all non-socket WSAEVENTs. When WSAWaitForMultipleEvents returns you have to iterate over dispatchers if the first WSAEVENT is signalled (which indicates socket activity). This is the point where you would try bringing keys into the picture, e.g. don\u0027t iterate over dispatchers_ but instead iterate over the array of keys that you saved earlier, when you called WSAEventSelect for a socket dispatcher. Check those keys against dispatchers_, similar to the select case.\n\nI\u0027m not sure if this is the most efficient way to do this on Windows. I don\u0027t know the reason for the complexity of multiplexing all sockets into one WSAEVENT. I suppose, it could be because of the WSA_MAXIMUM_WAIT_EVENTS limit. But I wonder if it would be more efficient to use one WSAEVENT per socket up until that limit is reached and thus reduce the need for iterating over all dispatchers. But this idea is for another patch.",
      "parentUuid": "46829394_a5878ad7",
      "revId": "b612c4ea3aadb2d8c5d3f76f27de5ba6bee99af4",
      "serverId": "58829da1-049c-39fb-b951-ebdcd0984767",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "2a5fd4db_b51b0ec7",
        "filename": "rtc_base/physical_socket_server.cc",
        "patchSetId": 9
      },
      "lineNbr": 1225,
      "author": {
        "id": 5053
      },
      "writtenOn": "2020-08-14T22:22:50Z",
      "side": 1,
      "message": "Good suggestion to use an array of keys.\n\nAnd you\u0027re probably right about WSA_MAXIMUM_WAIT_EVENTS. Looks like it\u0027s typically 64, which is far lower than the typical FD_SETSIZE, and is a limit WebRTC could potentially reach even with a single client, under the worst conditions.\n\nAlso I read the Windows code wrong, it actually does use different events for the wake up and socket events. So given the WSA_MAXIMUM_WAIT_EVENTS restriction, the code actually does make sense, though still would ideally use up to WSA_MAXIMUM_WAIT_EVENTS events.",
      "parentUuid": "b6551997_f1822ab7",
      "revId": "b612c4ea3aadb2d8c5d3f76f27de5ba6bee99af4",
      "serverId": "58829da1-049c-39fb-b951-ebdcd0984767",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "4b4acd2d_51c85ae4",
        "filename": "rtc_base/physical_socket_server.h",
        "patchSetId": 9
      },
      "lineNbr": 157,
      "author": {
        "id": 5053
      },
      "writtenOn": "2020-08-13T20:38:08Z",
      "side": 1,
      "message": "These three methods technically should have CamelCase naming according to Google style, but I thought keeping the naming scheme consistent with the C++ container methods makes more sense.",
      "revId": "b612c4ea3aadb2d8c5d3f76f27de5ba6bee99af4",
      "serverId": "58829da1-049c-39fb-b951-ebdcd0984767",
      "unresolved": false
    }
  ]
}