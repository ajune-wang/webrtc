{
  "comments": [
    {
      "key": {
        "uuid": "80c63202_dcb8aaee",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 9
      },
      "lineNbr": 0,
      "author": {
        "id": 5053
      },
      "writtenOn": "2020-08-13T20:38:08Z",
      "side": 1,
      "message": "PTAL.",
      "revId": "b612c4ea3aadb2d8c5d3f76f27de5ba6bee99af4",
      "serverId": "58829da1-049c-39fb-b951-ebdcd0984767",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "c18a0f7e_ba6c6b32",
        "filename": "rtc_base/physical_socket_server.cc",
        "patchSetId": 9
      },
      "lineNbr": 1225,
      "author": {
        "id": 5053
      },
      "writtenOn": "2020-08-13T20:38:08Z",
      "side": 1,
      "message": "We already have this mechanism to deal with dispatchers being added/removed during the select() loop. So I think this could be used for epoll as well; if a dispatcher is removed and a new one is added with the same address, the new one wouldn\u0027t be added until the next iteration, and any events produced by the old one would be ignored.\n\nHowever, we still need some custom structure to deal with the new test I added (because we need the ability to mark an entry as invalid without modifying the list). And I like the key implementation personally, it makes lookup O(1) and gives added peace of mind in case there\u0027s a case of the ABA problem I haven\u0027t thought of, or could be introduced in the future.",
      "revId": "b612c4ea3aadb2d8c5d3f76f27de5ba6bee99af4",
      "serverId": "58829da1-049c-39fb-b951-ebdcd0984767",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "0f976514_e8ad20f7",
        "filename": "rtc_base/physical_socket_server.cc",
        "patchSetId": 9
      },
      "lineNbr": 1225,
      "author": {
        "id": 13594
      },
      "writtenOn": "2020-08-13T22:11:16Z",
      "side": 1,
      "message": "\u003e We already have this mechanism to deal with dispatchers being added/removed during the select() loop. So I think this could be used for epoll as well; if a dispatcher is removed and a new one is added with the same address, the new one wouldn\u0027t be added until the next iteration, and any events produced by the old one would be ignored.\n\n1. The dispatched can be removed/added while the event loop thread has not yet locked crit_ after return from epoll_wait. processing_dispatchers_ is not set at that point. The reported events may be related to the dispatcher that has already been removed.\n\n2. We may hit the limit on the number of events reported from epoll_wait. So it is possible that events pertaining to a removed dispatcher will be delivered with a delay of one or more epoll_wait loop iterations. This makes the processing_dispatchers_ check ineffective.",
      "parentUuid": "c18a0f7e_ba6c6b32",
      "revId": "b612c4ea3aadb2d8c5d3f76f27de5ba6bee99af4",
      "serverId": "58829da1-049c-39fb-b951-ebdcd0984767",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "46829394_a5878ad7",
        "filename": "rtc_base/physical_socket_server.cc",
        "patchSetId": 9
      },
      "lineNbr": 1225,
      "author": {
        "id": 5053
      },
      "writtenOn": "2020-08-14T00:40:57Z",
      "side": 1,
      "message": "Good points. I tend to forget about the critical section because, as far as I know, WebRTC only interacts with sockets on one thread.\n\nJust curious, is #2 a problem even if we do EPOLL_CTL_DEL with the old socket before calling epoll_wait again?\n\nAlso, unless I\u0027m mistaken, we still have problem #2 with the normal select loop; a socket could be closed and a new one created with the same handle during the select() call. To address this, we could add an \"added_to_fd_set\" flag to Dispatcher which would only be set to true once FD_SET is called. Can you think of anything more elegant?\n\nGuess this is a problem for Windows as well. Though now that I\u0027m finally looking at it, I don\u0027t understand the Windows code *at all*. All actual sockets will return WSA_INVALID_EVENT for GetWSAEvent, yet they\u0027re still added to the event list. Instead of each socket having its own event, WSAEventSelect is called with events[0] for every socket. So WSAWaitForMultipleEvents is actually only waiting for a single event which indicates \"WakeUp was called or something happened to one of the sockets\". Then *all* sockets are iterated (even if WakeUp was called and nothing actually happened to any socket!).\n\nIs this is busted as it seems? Been a while since I worked with Windows sockets but this seems to confirm: https://stackoverflow.com/questions/19936131/wsaeventselect-one-event-multiple-sockets",
      "parentUuid": "0f976514_e8ad20f7",
      "revId": "b612c4ea3aadb2d8c5d3f76f27de5ba6bee99af4",
      "serverId": "58829da1-049c-39fb-b951-ebdcd0984767",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "4b4acd2d_51c85ae4",
        "filename": "rtc_base/physical_socket_server.h",
        "patchSetId": 9
      },
      "lineNbr": 157,
      "author": {
        "id": 5053
      },
      "writtenOn": "2020-08-13T20:38:08Z",
      "side": 1,
      "message": "These three methods technically should have CamelCase naming according to Google style, but I thought keeping the naming scheme consistent with the C++ container methods makes more sense.",
      "revId": "b612c4ea3aadb2d8c5d3f76f27de5ba6bee99af4",
      "serverId": "58829da1-049c-39fb-b951-ebdcd0984767",
      "unresolved": false
    }
  ]
}